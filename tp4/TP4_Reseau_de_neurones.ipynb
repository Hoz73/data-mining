{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"TP4_Reseau_de_neurones.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyODQ9HyusU2bKTL1Aic1wxZ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# TP4: Introduction aux réseaux de neurones\n","Pour ce TP, nous allons utiliser la librairie PyTorch."],"metadata":{"id":"-xParNstTmHV"}},{"cell_type":"markdown","source":["---\n","Le code qui suit est une fonction qui ne sert qu'à dessiner les réseaux de neurones. **Vous n'avez pas besoin d'en prendre connaissance**. Il faut run la cellule puis passer directement à la partie **Tensors**"],"metadata":{"id":"51LOgiV-Dmvv"}},{"cell_type":"code","source":["# Heavily based on https://github.com/Prodicode/ann-visualizer\n","\n","def ann_viz(model, view=True, filename=\"network.gv\"):\n","    \"\"\"Vizualizez a Sequential model.\n","\n","    # Arguments\n","        model: A Keras model instance.\n","\n","        view: whether to display the model after generation.\n","\n","        filename: where to save the vizualization. (a .gv file)\n","\n","        title: A title for the graph\n","    \"\"\"\n","    from graphviz import Digraph\n","\n","    input_layer = 0\n","    hidden_layers_nr = 0\n","    layer_types = []\n","    hidden_layers = []\n","    output_layer = 0\n","    layers = [layer for layer in model.modules() if type(layer) == torch.nn.Linear]\n","\n","    for layer in layers:\n","        if layer == layers[0]:\n","            input_layer = layer.in_features\n","            hidden_layers_nr += 1\n","            if type(layer) == torch.nn.Linear:                \n","                hidden_layers.append(layer.out_features)\n","                layer_types.append(\"Dense\")\n","            else:\n","                raise Exception(\"Input error\")\n","\n","        else:\n","            if layer == layers[-1]:\n","                output_layer = layer.out_features\n","            else:\n","                hidden_layers_nr += 1\n","                if type(layer) == torch.nn.Linear:\n","\n","                    hidden_layers.append(layer.out_features)\n","                    layer_types.append(\"Dense\")\n","                else:\n","                    raise Exception(\"Hidden error\")\n","        last_layer_nodes = input_layer\n","        nodes_up = input_layer\n","\n","    g = Digraph(\"g\", filename=filename)\n","    n = 0\n","    g.graph_attr.update(splines=\"false\", nodesep=\"0.5\", ranksep=\"0\", rankdir='LR')\n","    # Input Layer\n","    with g.subgraph(name=\"cluster_input\") as c:\n","        if type(layers[0]) == torch.nn.Linear:\n","            the_label = \"Input Layer\"\n","            if layers[0].in_features > 10:\n","                the_label += \" (+\" + str(layers[0].in_features - 10) + \")\"\n","                input_layer = 10\n","            c.attr(color=\"white\")\n","            for i in range(0, input_layer):\n","                n += 1\n","                c.node(str(n))\n","                c.attr(labeljust=\"1\")\n","                c.attr(label=the_label, labelloc=\"bottom\")\n","                c.attr(rank=\"same\")                \n","                c.node_attr.update(\n","                    width=\"0.65\",\n","                    style=\"filled\",                    \n","                    shape=\"circle\",\n","                    color=HAPPY_COLORS_PALETTE[3],\n","                    fontcolor=HAPPY_COLORS_PALETTE[3],\n","                )\n","    for i in range(0, hidden_layers_nr):\n","        with g.subgraph(name=\"cluster_\" + str(i + 1)) as c:\n","            if layer_types[i] == \"Dense\":\n","                c.attr(color=\"white\")\n","                c.attr(rank=\"same\")\n","                the_label = f'Hidden Layer {i + 1}'\n","                if layers[i].out_features > 10:\n","                    the_label += \" (+\" + str(layers[i].out_features - 10) + \")\"\n","                    hidden_layers[i] = 10\n","                c.attr(labeljust=\"right\", labelloc=\"b\", label=the_label)\n","                for j in range(0, hidden_layers[i]):\n","                    n += 1\n","                    c.node(\n","                        str(n),\n","                        width=\"0.65\",\n","                        shape=\"circle\",\n","                        style=\"filled\",\n","                        color=HAPPY_COLORS_PALETTE[0],\n","                        fontcolor=HAPPY_COLORS_PALETTE[0],\n","                    )\n","                    for h in range(nodes_up - last_layer_nodes + 1, nodes_up + 1):\n","                        g.edge(str(h), str(n))\n","                last_layer_nodes = hidden_layers[i]\n","                nodes_up += hidden_layers[i]\n","            else:\n","                raise Exception(\"Hidden layer type not supported\")\n","\n","    with g.subgraph(name=\"cluster_output\") as c:\n","        if type(layers[-1]) == torch.nn.Linear:\n","            c.attr(color=\"white\")\n","            c.attr(rank=\"same\")\n","            c.attr(labeljust=\"1\")\n","            for i in range(1, output_layer + 1):\n","                n += 1\n","                c.node(\n","                    str(n),\n","                    width=\"0.65\",\n","                    shape=\"circle\",\n","                    style=\"filled\",\n","                    color=HAPPY_COLORS_PALETTE[4],\n","                    fontcolor=HAPPY_COLORS_PALETTE[4],\n","                    \n","                )\n","                for h in range(nodes_up - last_layer_nodes + 1, nodes_up + 1):\n","                    g.edge(str(h), str(n))\n","            c.attr(label=\"Output Layer\", labelloc=\"bottom\")\n","            c.node_attr.update(\n","                color=\"#2ecc71\", style=\"filled\", fontcolor=\"#2ecc71\", shape=\"circle\"\n","            )\n","\n","    g.attr(arrowShape=\"none\")\n","    g.edge_attr.update(arrowhead=\"none\", color=\"#707070\", penwidth=\"2\")\n","    if view is True:\n","        g.view()\n","\n","    return g"],"metadata":{"id":"DXzk31z_d-_2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","## 1. Tensors"],"metadata":{"id":"nDP8YLqZXOO9"}},{"cell_type":"markdown","source":["1. Importez torch et numpy :"],"metadata":{"id":"4MwfBnRqrk_M"}},{"cell_type":"code","source":["# Import libraries\n"],"metadata":{"id":"2B7fkW2ZQJ3Z"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["2. Transformez *data* en tensor :"],"metadata":{"id":"Uv3iNbaBrnb0"}},{"cell_type":"code","source":["# Create some data\n","data = [[1, 2],[3, 4]]\n","\n","# From list to tensor\n"],"metadata":{"id":"lKs_PGQuRXDF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["3. Transformez *data* en array puis de array en tensor"],"metadata":{"id":"2sJ7YK1Xrra0"}},{"cell_type":"code","source":["# From list to array\n","\n","# From array to tensor\n"],"metadata":{"id":"1bVXSe5jRaXk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["4. Créez un tensor avec des valeurs aléatoires (**torch.rand()**) ayant une forme (3,4). Puis affichez ses valeurs, son type (**.dtype**) et le device (**.device**) sur lequel est enregistré ce dernier."],"metadata":{"id":"TQvzZYsKru9l"}},{"cell_type":"code","source":["# Create random tensor of shape (3, 4)\n"],"metadata":{"id":"8WezI25dRbDr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["5. Créez 2 tensors:\n","[[10, 20], [30, 40]]\n","et [[2, 2], [1, 5]].  \n","Faites en la somme puis la multiplication."],"metadata":{"id":"uSqJfIl6hzbm"}},{"cell_type":"code","source":["# Create 2 tensor\n","\n","# Elementwise sum and mult\n"],"metadata":{"id":"jlj4QHxCSSQB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["6. Transformez le résultat de la multiplication en array"],"metadata":{"id":"RW-nyCQ6h3FF"}},{"cell_type":"code","source":["# From tensor to array\n"],"metadata":{"id":"ZYPk1WAGTOPr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","## 2. Build your own MLP\n"],"metadata":{"id":"vD3cX5i2D6dG"}},{"cell_type":"code","source":["import torch\n","import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import confusion_matrix, classification_report\n","import seaborn as sns\n","from pylab import rcParams\n","import matplotlib.pyplot as plt\n","\n","from torch import nn, optim\n","import torch.nn.functional as F\n","\n","\n","%matplotlib inline\n","%config InlineBackend.figure_format='retina'\n","\n","# Display graph parameters\n","sns.set(style='whitegrid', palette='muted', font_scale=1.2)\n","HAPPY_COLORS_PALETTE = [\"#01BEFE\", \"#FFDD00\", \"#FF7D00\", \"#FF006D\", \"#93D30C\", \"#8F00FF\"]\n","sns.set_palette(sns.color_palette(HAPPY_COLORS_PALETTE))\n","rcParams['figure.figsize'] = 12, 8\n","\n","# Random seed parameters\n","RANDOM_SEED = 42\n","np.random.seed(RANDOM_SEED)\n","torch.manual_seed(RANDOM_SEED)"],"metadata":{"id":"aaijqtE-iwu9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Préparation des données\n","Pour tout projet de data science, la première partie, et souvent la plus fastidieuse, est la préparation des données. Une fois cette étape validée, vous pourrez utiliser ces dernières dans un réseau de neurones."],"metadata":{"id":"xVQAmEzQgZoz"}},{"cell_type":"markdown","source":["Téléchargez les données au lien suivant: [https://www.kaggle.com/jsphyg/weather-dataset-rattle-package](https://www.kaggle.com/jsphyg/weather-dataset-rattle-package)  \n","Créez un dossier Data/ et importer les données dans ce dernier.  \n","Dézippez le fichier dans le dossier Data:"],"metadata":{"id":"4memL-DVrzdg"}},{"cell_type":"code","source":["# Unzip file containing data\n","# !unzip  Data/archive.zip -d Data/"],"metadata":{"id":"NQZKZp-3hhdK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["7. Créez le dataframe *df* à partir des données *weatherAUS.csv*"],"metadata":{"id":"jiZ8l7WOiI37"}},{"cell_type":"code","source":["# Read data\n"],"metadata":{"id":"nHA7QQ48hhGo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["8. Affichez le nombre de lignes puis de colonnes du dataframe"],"metadata":{"id":"HreH92B7iMev"}},{"cell_type":"code","source":["# Number of rows\n","\n","# Number of columns\n"],"metadata":{"id":"LxJt4B6Vhg6G"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["9. Sélectionnez uniquement les colonnes de *df* qui nous intéresse:  \n","['Rainfall', 'Humidity3pm', 'Pressure9am', 'RainToday', 'RainTomorrow']"],"metadata":{"id":"EzanVS-UiRsG"}},{"cell_type":"code","source":["# Select only relevant columns\n"],"metadata":{"id":"zJ9fTBKtjHtX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["10. Checkez les valeurs manquantes puis supprimez les lignes contenant des NaN."],"metadata":{"id":"piMZrzdXiUuw"}},{"cell_type":"code","source":["# Print NaN\n","\n","# Drop NaN\n"],"metadata":{"id":"5AyNzQNueZX0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Dans cette exemple, nous avons assez d'observations à étudier, on va se contenter de supprimer toutes les lignes où des NaN sont présents.\n","**Note:** cette manière de pratiquer n'est pas très académique mais le but du TP est autre."],"metadata":{"id":"rfs1lCGzkXun"}},{"cell_type":"markdown","source":["11. Les modèles de machine learning n'acceptent que des données numériques. Il faut donc transformer les valeurs qualitatives en quantitatives des colonnes RainToday et RainTomorrow:  \n","- Yes = 1\n","- No = 0\n","\n","S'assurer que les colonnes soient bien de type **int**."],"metadata":{"id":"V9jgi-0Jngap"}},{"cell_type":"code","source":["# Transform string into binary int values\n"],"metadata":{"id":"PPRFz0WkjHpy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Ensuite, on va découper notre jeu de données en 2 datasets:  \n","- 1 pour l'entraînement du réseau. Que l'on découpera ensuite en 2 autres datasets (train/validation)\n","- 1 pour tester notre modèle (test). Ce dernier va nous permettre de calculer les performances de notre modèle sur des données nouvelles, pas utilisée lors de l'entraînement."],"metadata":{"id":"qao8sQlInxWO"}},{"cell_type":"markdown","source":["12. Utilisez **train_test_split** pour créer deux datasets à partir de *df*: *df_train* et *df_test*"],"metadata":{"id":"E2W_vx8eijSg"}},{"cell_type":"code","source":["# Split df into train/validation set (80%/20%) with random_state=RANDOM_SEED\n"],"metadata":{"id":"il0ifVSgq8dq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Un des problèmes classique en machine learning est le déséquilibre de classe. Ce problème est souvent à l'origine de piètres performances des modèles. On va regarder ce qu'il en est pour notre jeu de données:"],"metadata":{"id":"j6jTOGxFmRFE"}},{"cell_type":"markdown","source":["13. Tracez un barplot qui représente la distribution des valeurs de la colonne *df_train.RainTomorrow*. Vous pouvez facilement y parvenir grâce à *sns.countplot()*"],"metadata":{"id":"p9Fs2W6cimrd"}},{"cell_type":"code","source":["# Countplot\n"],"metadata":{"id":"nn-ywgK_lFdf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["14. Déterminer le pourcentage de 0 et de 1 dans cette même colonnne *df_train.RainTomorrow*"],"metadata":{"id":"ihxBzXUeiuQU"}},{"cell_type":"code","source":["# In percent, we get:\n"],"metadata":{"id":"8_eNXWp4p934"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Notre dataset est énormément déséquilibré! (78%/22%).  \n"," On va appliquer la technique dites d'oversampling (simple). C'est à dire augmenter la quantité de lignes ayant pour valeur cible 1 pour en avoir que de 1. Ici, on ajoute simplement à df_train des lignes où RainTomorrow == 1."],"metadata":{"id":"xivMcFzjn1Oq"}},{"cell_type":"code","source":["# Oversampling\n","df_train = pd.concat([df_train, df_train[df_train.RainTomorrow == 1], df_train[df_train.RainTomorrow == 1]], axis=0)"],"metadata":{"id":"m8p-cN_2o5FD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["15. Tracez l'histogramme qui montre la nouvelle distribution de RainTomorrow et calculez le pourcentage de 0 et de 1."],"metadata":{"id":"jubqWglTi023"}},{"cell_type":"code","source":["# Print again the new balance between rain and no rain\n"],"metadata":{"id":"DCYVGN85q8fh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# In percent, we get:\n"],"metadata":{"id":"SckuiYTdposM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Maintenant que nos données sont à peu près équilibrées (53%/46%), il faut découper notre jeu de données en jeu de train et validation."],"metadata":{"id":"8uuzPdSErQai"}},{"cell_type":"markdown","source":["16. Découpez en deux dataset le dataframe df_train (80% de données en train et 20% en validation). Vous pouvez à nouveau utiliser la fonction **train_test_split**, mais cette fois avec df_train et les colonnes pertinentes. df_train[X] pour les variables et df_train[y] pour la valeur cible."],"metadata":{"id":"7-3D5jDTi5kI"}},{"cell_type":"code","source":["# Split df_train in train/test set (80%/20%) with random_state=RANDOM_SEED\n"],"metadata":{"id":"y5G6dQoGq8Zk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["17. Convertissez X_train, X_test, y_train, y_test en array (*.to_numpy*) puis de array en tensor (*torch.from_numpy*) sous la forme de float (*.float*). Attention les *y_* doivent être *.squeeze*."],"metadata":{"id":"Lsm01Sg4i-X-"}},{"cell_type":"code","source":["# Convert data from numpy to float tensor\n"],"metadata":{"id":"uoK0jxxii_TE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Multi Layer Perceptron"],"metadata":{"id":"T1NOO6qdfqxX"}},{"cell_type":"markdown","source":["#### Create the network"],"metadata":{"id":"UcRoSFY8jJmo"}},{"cell_type":"markdown","source":["Dans cette partie, vous allez utiliser un MLP pour réaliser les prédictions de pluie à l'aide de la variable RainTomorrow. Vous pouvez observer dans la classe MLP les différentes couches de neurones, leurs dimensions et les fonctions d'activations utilisées."],"metadata":{"id":"OWcprwuEuDKJ"}},{"cell_type":"code","source":["# Définir la classe MLP\n","class MLP(nn.Module):\n","\n","  def __init__(self, n_features):\n","    super(MLP, self).__init__()\n","    self.fc1 = nn.Linear(n_features, 5)\n","    self.fc2 = nn.Linear(5, 3)\n","    self.fc3 = nn.Linear(3, 1)\n","\n","  def forward(self, x):\n","    x = F.relu(self.fc1(x))\n","    x = F.relu(self.fc2(x))\n","    return torch.sigmoid(self.fc3(x))"],"metadata":{"id":"c0kw3K2LfrUF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Initialize the network\n","model = MLP(X_train.shape[1])"],"metadata":{"id":"id7WGwHIjXIU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Print our network\n","ann_viz(model, view=False)"],"metadata":{"id":"8z0pBQz9uBQp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["On peut aussi afficher la structure du modèle:"],"metadata":{"id":"4wZ_FJ1qjesC"}},{"cell_type":"code","source":["# Check model structure\n","print(\"Model structure: \", model, \"\\n\\n\")\n","\n","for name, param in model.named_parameters():\n","    print(f\"Layer: {name} | Size: {param.size()} | Values : {param[:2]} \\n\")"],"metadata":{"id":"nLQ9RukGBgwL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["18. Maintenant que le réseau est créé, vous allez définir ses hyperparamètres.   \n","On veut utiliser la fonction de perte **nn.BCELoss** et l'optimiseur **optim.SGD** avec un learning rate de 0.001"],"metadata":{"id":"_7U2QzFAjiT8"}},{"cell_type":"code","source":["## Hyperparameters\n","\n","# Choose the loss function\n","\n","# Choose the optimizer\n"],"metadata":{"id":"DdH5TclOu6oO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Ensuite on définit quelques fonctions et listes..."],"metadata":{"id":"1sbzFNAOjvA0"}},{"cell_type":"code","source":["# Define a wee function to compute accuracy\n","def calculate_accuracy(y_true, y_pred):\n","  predicted = y_pred.ge(.5).view(-1)\n","  return (y_true == predicted).sum().float() / len(y_true)\n","\n","# Define a function to round numbers\n","def round_tensor(t, decimal_places=3):\n","  return round(t.item(), decimal_places)"],"metadata":{"id":"ZTtm6B9Dvdqw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create some empty lists to pick up some values during training\n","list_train_loss = []\n","list_val_loss = []\n","list_train_acc = []\n","list_val_acc = []"],"metadata":{"id":"RwYfau24zgnz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Training"],"metadata":{"id":"gd8Sd0lO-FnA"}},{"cell_type":"markdown","source":["19. C'est ici que l'entraînement du modèle va avoir lieu. Les données seront lues 1000 fois (1000 epochs). Vous allez devoir compléter les lignes de codes et permettre l'entraînement du modèle."],"metadata":{"id":"CLB-5AtIj0sb"}},{"cell_type":"code","source":["############## TRAINING ##############\n","for epoch in range(1000):\n","    \n","    # Do prediction on trainset\n","    y_pred = # Here\n","    y_pred = torch.squeeze(y_pred)\n","\n","    # Compute loss\n","    train_loss = # Here\n","    \n","    # For some epoch compute val loss and accuracy\n","    if epoch % 100 == 0:\n","      # Accuracy between train and true\n","      train_acc = # Here\n","      \n","      # Prediction on X_validation\n","      y_val_pred = # Here\n","      y_val_pred = torch.squeeze(y_val_pred)\n","\n","      # Compute loss and accuracy on validation\n","      val_loss = # Here\n","      val_acc = # Here\n","\n","      # Save train/val loss/accuracy\n","      list_train_loss.append(round_tensor(train_loss))\n","      list_val_loss.append(round_tensor(val_loss))\n","      list_train_acc.append(round_tensor(train_acc))\n","      list_val_acc.append(round_tensor(val_acc))\n","\n","      # Print some informations\n","      print(\n","f'''epoch {epoch}\n","Train set - loss: {round_tensor(train_loss)}, accuracy: {round_tensor(train_acc)}\n","Validation  set - loss: {round_tensor(val_loss)}, accuracy: {round_tensor(val_acc)}\n","''')\n","    \n","    # All gradients to zero (avoid exploding gradient)\n","    # Here\n","\n","    # Backproapagation\n","    # Here\n","    \n","    # Update weights\n","    # Here"],"metadata":{"id":"rN3lOHvIvFJO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["20. Afin d'éviter l'overfitting ou l'underfitting de notre modèle, vous allez analyser les valeurs de perte que relevées pour le trainset et validation set. Utilisez les variables *list_train_loss* et *list__loss* et tracez leur variation en fonction de leur epoch associée. **plt.plot()**"],"metadata":{"id":"na9X7mzrj8Ix"}},{"cell_type":"code","source":["# Check overfitting (Train Loss vs Test Loss)\n"],"metadata":{"id":"x2cj5E9MwmOT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Test\n","Le test est une étape d'inférence. Le modèle maintenant entraîné va être capable de faire des prédictions sur des données qu'il n'a jamais vu. Nous pourrons alors déterminer son efficacité."],"metadata":{"id":"vubQoEu_96pi"}},{"cell_type":"markdown","source":["21. Vous devez donc créer 2 variables issues de *df_test*: *X_test* et *y_test*. Comme précédemment, convertir d'abord *df_test[X]* en array puis en tensor et en float. Pareil pour *df_test[y]*.  \n","Puis faire les prédiction à l'aide du modèle sur les données *X_test*."],"metadata":{"id":"jVwlKUuwkYhM"}},{"cell_type":"code","source":["# Convert data from numpy to tensor\n"],"metadata":{"id":"mb0wo_PKyNAS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["22. Utiliser la fonction **classification_report** pour obtenir toutes les métriques qui vous aideront à qualifier les performances de votre modèle."],"metadata":{"id":"4aE0AhDHkkcD"}},{"cell_type":"code","source":["# Do prediction on testset\n"],"metadata":{"id":"cBoKKqJokleU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Confusion Matrix"],"metadata":{"id":"UOgdYsLBF-jA"}},{"cell_type":"markdown","source":["23. Ici, je vous ai créé la matrice de confusion associée aux résultats du test. Conclure sur l'efficacité de votre modèle:"],"metadata":{"id":"1FYowJZTkstc"}},{"cell_type":"code","source":["cm = confusion_matrix(y_test, y_pred)\n","df_cm = pd.DataFrame(cm, index=classes, columns=classes)\n","\n","hmap = sns.heatmap(df_cm, annot=True, fmt=\"d\")\n","hmap.yaxis.set_ticklabels(hmap.yaxis.get_ticklabels(), rotation=0, ha='right')\n","hmap.xaxis.set_ticklabels(hmap.xaxis.get_ticklabels(), rotation=30, ha='right')\n","plt.ylabel('True label')\n","plt.xlabel('Predicted label');"],"metadata":{"id":"16QP1gDEzL4X"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(f'Number of \"No rain\" values: {np.count_nonzero(y_test.numpy() == 0)}')\n","print(f'Number of \"Raining\" values: {np.count_nonzero(y_test.numpy() == 1)}')"],"metadata":{"id":"3YZfdtpmwxhS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Note:** avant de passer à la suite du TP, vous pouvez essayer de modifier l'architecture du MLP (nombre de layers, fonction d'activations, ...) que vous avez utilisé pour faire la prédiction et relancer le code en entier. Vous pourrez alors observer des variations de performances."],"metadata":{"id":"yPRScCCysdlO"}},{"cell_type":"markdown","source":["## 3. A vous de jouer !"],"metadata":{"id":"Qlc01SKMhTov"}},{"cell_type":"markdown","source":["Télécharger l'archive au [lien](https://www.kaggle.com/iabhishekofficial/mobile-price-classification/download) suivant et placez là dans un dossier Data2 que vous créerez.  \n","Après avoir dézippé l'archive, vous trouverez un jeu de données de train et un de test. Vous trouverez une description des données [ici](https://www.kaggle.com/iabhishekofficial/mobile-price-classification?select=train.csv)."],"metadata":{"id":"jaOhDdVUmBq2"}},{"cell_type":"markdown","source":["24. Sur le même principe que pour la partie **Build your own MLP**, vous devez créer un réseau de neurones multi-couche afin de prédire la classe de prix des téléphones du jeu de test. Vous êtes libre dans la préparation des données, le choix des variables et des paramètres (optimizer, learning rate, fonction d'activation, ...).  \n","Utilisez la fonction [**classification_report()**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html) pour afficher les résultats de votre prédiction."],"metadata":{"id":"rLtL4ryInXdy"}},{"cell_type":"code","source":[""],"metadata":{"id":"D8IOK-q4njRj"},"execution_count":null,"outputs":[]}]}